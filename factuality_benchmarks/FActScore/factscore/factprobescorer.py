import string
import json
import re
import os
import logging
import pickle
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from factscore.atomic_facts_ext import AtomicFactGenerator
from factscore.openai_lm import OpenAIModel

# Consolidated exclusion patterns using regex alternation and common keywords
exclusion_patterns = re.compile(
    r'\b(?:I\s+(?:cannot|was unable to|do not|have no)\s+(?:find|provide|locate|access)|'
    r'(?:No|Insufficient|Cannot|Unable)\s+(?:information|data|details|records|bio|profile|content)|'
    r'(?:I\s+am\s+not\s+sure|Note\s+(?:that|,))|'
    r'(?:No\s+(?:additional|further|relevant)\s+information))\b',
    flags=re.IGNORECASE
)

# Constants for relevance classification
SYMBOL = 'Foo'
NOT_SYMBOL = 'Not Foo'

_PROMPT_PLACEHOLDER = '[PROMPT]'
_RESPONSE_PLACEHOLDER = '[RESPONSE]'
_STATEMENT_PLACEHOLDER = '[ATOMIC FACT]'

_RELEVANCE_FORMAT = f"""\
In a given RESPONSE, two subjects are considered "{SYMBOL}" if the RESPONSE \
contains information that explains how the two subjects are related.

Instructions:
1. The following STATEMENT has been extracted from the broader context of the \
given RESPONSE to the given QUESTION.
2. First, state the broad subject of the STATEMENT and the broad subject of the \
QUESTION.
3. Next, determine whether the subject of the STATEMENT and the subject of the \
QUESTION should be considered {SYMBOL}, based on the given definition of \
"{SYMBOL}."
4. Before showing your answer, think step-by-step and show your specific \
reasoning.
5. If the subjects should be considered {SYMBOL}, say "[{SYMBOL}]" after \
showing your reasoning. Otherwise show "[{NOT_SYMBOL}]" after showing your \
reasoning.
6. Your task is to do this for the STATEMENT and RESPONSE under "Your Task". \
Some examples have been provided for you to learn how to do this task.

... (Examples omitted for brevity)

Your Task:
QUESTION:
{_PROMPT_PLACEHOLDER}

RESPONSE:
{_RESPONSE_PLACEHOLDER}

STATEMENT:
{_STATEMENT_PLACEHOLDER}
"""
_REVISE_FORMAT = f"""\
Vague references include but are not limited to:
- Pronouns (e.g., "his", "they", "her")
- Unknown entities (e.g., "this event", "the research", "the invention")
- Non-full names (e.g., "Jeff..." or "Bezos..." when referring to Jeff Bezos)

Instructions:
1. The following STATEMENT has been extracted from the broader context of the \
given RESPONSE.
2. Modify the STATEMENT by replacing vague references with the proper entities \
from the RESPONSE that they are referring to.
3. You MUST NOT change any of the factual claims made by the original STATEMENT.
4. You MUST NOT add any additional factual claims to the original STATEMENT. \
For example, given the response "Titanic is a movie starring Leonardo \
DiCaprio," the statement "Titanic is a movie" should not be changed.
5. Before giving your revised statement, think step-by-step and show your \
reasoning. As part of your reasoning, be sure to identify the subjects in the \
STATEMENT and determine whether they are vague references. If they are vague \
references, identify the proper entity that they are referring to and be sure \
to revise this subject in the revised statement.
6. After showing your reasoning, provide the revised statement and wrap it in \
a markdown code block.
7. Your task is to do this for the STATEMENT and RESPONSE under "Your Task". \
Some examples have been provided for you to learn how to do this task.

... (Examples omitted for brevity)

Your Task:
STATEMENT:
{_STATEMENT_PLACEHOLDER}

RESPONSE:
{_RESPONSE_PLACEHOLDER}
"""

# Utility functions for extracting information from model responses
def extract_first_square_brackets(text):
    match = re.search(r'\[([^\[\]]+)\]', text)
    result = match.group(1) if match else None
    logging.debug(f"Extracted from square brackets: {result}")
    return result

def extract_first_code_block(text, ignore_language=False):
    if not isinstance(text, str):
        logging.error(f"Expected string in extract_first_code_block, got {type(text)}")
        return None
    pattern = r'```(?:[\w\+\-\.]+\n)?([\s\S]+?)```' if ignore_language else r'```[\w\+\-\.]+\n([\s\S]+?)```'
    match = re.search(pattern, text)
    result = match.group(1).strip() if match else None
    logging.debug(f"Extracted code block: {result}")
    return result

def strip_string(s):
    result = s.strip()
    logging.debug(f"Stripped string: {result}")
    return result

def post_process_generation(generated_text: str) -> str:
    """
    Post-process the generated text to ensure completeness and remove extraneous content.
    
    Args:
        generated_text (str): The raw text generated by the model.
    
    Returns:
        str: The cleaned and validated text.
    """
    # Ensure the text ends with a period. If not, truncate to the last complete sentence.
    if not generated_text.endswith('.'):
        last_period = generated_text.rfind('.')
        if last_period != -1:
            generated_text = generated_text[:last_period + 1]
    
    # Remove explanations about inability to find more information using the consolidated pattern
    gen = exclusion_patterns.split(generated_text)[0].strip()
    
    return gen

class FactProbeScorer(object):

    def __init__(self,
                 model_name="retrieval+ChatGPT",
                 data_dir=None,
                 model_dir=None,
                 cache_dir=None,
                 openai_key="api.key",
                 cost_estimate="consider_cache",
                 abstain_detection_type=None,
                 batch_size=256,
                 probe_path=None,
                 hf_model_name='meta-llama/Meta-Llama-3.1-8B-Instruct',
                 device='cuda',
                 max_memory='80GIB'):
        if data_dir is None:
            data_dir = os.environ.get("FACTSCORE_CACHE_DIR", "./cache/factscore")
        if cache_dir is None:
            cache_dir = os.environ.get("FACTSCORE_CACHE_DIR", "./cache/factscore")
        
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(cache_dir, exist_ok=True)
        
        self.model_name = model_name

        self.batch_size = batch_size
        self.openai_key = openai_key
        self.abstain_detection_type = abstain_detection_type

        self.data_dir = data_dir
        self.cache_dir = cache_dir

        self.af_generator = None

        self.device = device

        # Initialize the Language Model
        logging.debug("Initializing Language Model...")
        self.tokenizer, self.model = self.initialize_model(hf_model_name, device=device, max_memory=max_memory)
        self.model.eval()

        # Load the trained probes and other information from a pickle file
        self.probe = None
        self.classifier_name = None
        self.layer_group = None
        self.token_pos = 'lt'  # Default token position
        self.C = None
        self.max_iter = None

        self.probe_path = probe_path
        if probe_path:
            logging.debug(f"Loading trained probes and configurations from {probe_path}...")
            with open(probe_path, 'rb') as f:
                probe_data = pickle.load(f)
                self.probe = probe_data.get('probe')
                self.classifier_name = probe_data.get('classifier_name')
                self.layer_group = probe_data.get('layer_group')  # e.g., (13, 17)
                self.token_pos = probe_data.get('token_pos', 'lt')  # e.g., 'lt' or 'slt'
                self.C = probe_data.get('C')
                self.max_iter = probe_data.get('max_iter')
                
                if not self.probe or not self.classifier_name:
                    logging.error("Probe file must contain both 'probe' and 'classifier_name'.")
                    raise ValueError("Incomplete probe data in the probe file.")
                else:
                    logging.info(f"Loaded probe type: {self.classifier_name}")
        else:
            logging.error("Probe path not provided or file not found. Cannot proceed without a trained probe.")
            raise ValueError("Probe path must be provided.")

        # Initialize the model for relevance classification and fact refinement
        logging.debug("Initializing relevance and refinement model...")
        self.relevance_lm = OpenAIModel("ChatGPT",
                                        cache_file=os.path.join(cache_dir, "RelevanceChatGPT.pkl"),
                                        key_path=openai_key)

    def initialize_model(self, hf_model_name, device='cuda', max_memory='80GIB'):
        tokenizer = AutoTokenizer.from_pretrained(hf_model_name)
        model = AutoModelForCausalLM.from_pretrained(
            hf_model_name,
            device_map='auto',
            torch_dtype=torch.float16,
            max_memory={0: max_memory}
        )
        logging.info(f'Model "{hf_model_name}" loaded on device "{device}".')
        return tokenizer, model

    def set_probe_type(self, probe_type):
        """
        Set the probe type based on the classifier name loaded from the probe file.
        
        Args:
            probe_type (str): 'Logistic Regression' or 'XGBoost'.
        """
        if probe_type not in ['Logistic Regression', 'XGBoost']:
            raise ValueError("probe_type must be either 'Logistic Regression' or 'XGBoost'.")
        if self.classifier_name != probe_type:
            raise ValueError(f"Probe file contains '{self.classifier_name}' classifier. Cannot set to '{probe_type}'.")
        logging.info(f"Probe type confirmed as: {self.probe_type}")
        self.probe_type = probe_type

    def process_generation_yield(self, topic, generation):
        """Process a single generation and return structured results."""
        # Extract atomic claims and token indices
        atomic_facts_with_spans = self.extract_atomic_facts(generation)
        prior_sent = None
        sent_facts = []

        for atom_data in atomic_facts_with_spans:
            atom_text = atom_data['fact'].strip()
            token_indices = atom_data['token_indices']
            sentence = atom_data['sent_text']

            # Step 1: Revise the atomic fact
            revised_atom = self.revise_fact(generation, atom_text)
            logging.info(f"Revised atomic fact: {revised_atom}")

            # Step 2: Check relevance
            is_relevant = self.check_relevance(topic, sentence, revised_atom)
            logging.info(f"Is relevant: {is_relevant}")

            # Skip irrelevant facts if necessary
            # if not is_relevant:
            #     logging.info("Atomic fact is irrelevant, skipping...")
            #     continue

            # Get predicted score from the probe
            is_supported, support_prob = self.predict_support(revised_atom)

            # Collect the result in a structured format
            result = {
                'sentence': sentence,
                'original_atomic_claim': atom_text,
                'revised_atomic_claim': revised_atom,
                'token_indices': token_indices,
                'is_relevant': is_relevant,  
                'support_probability': support_prob,
                'is_supported': is_supported
            }

            # Check if the current sentence is different from the prior sentence
            if prior_sent and prior_sent != sentence:
                # Yield the facts of the prior sentence
                yield sent_facts, prior_sent
                # Reset the sent_facts for the new sentence
                sent_facts = [result]
                prior_sent = sentence
            else:
                # Append the result to sent_facts
                sent_facts.append(result)
                # Set the prior_sent if it's not already set
                if not prior_sent:
                    prior_sent = sentence

        # After the loop ends, yield any remaining facts
        if sent_facts and prior_sent:
            yield sent_facts, prior_sent

    def process_generation(self, topic, generation):
        """Process a single generation and return structured results."""
        all_results = []
        atomic_facts_with_spans = self.extract_atomic_facts(generation)
        for atom_data in atomic_facts_with_spans:
            atom_text = atom_data['fact'].strip()
            token_indices = atom_data['token_indices']
            sentence = atom_data['sent_text']

            # Step 1: Revise the atomic fact
            revised_atom = self.revise_fact(sentence, atom_text)
            logging.info(f"Revised atomic fact: {revised_atom}")

            # Step 2: Check relevance
            is_relevant = self.check_relevance(topic, sentence, revised_atom)
            logging.info(f"Is relevant: {is_relevant}")

            if not is_relevant:
                # Skip irrelevant facts
                logging.info("Atomic fact is irrelevant, skipping...")
                continue

            # Get predicted score from the probe
            is_supported, support_prob = self.predict_support(revised_atom)
            
            print(f"Sentence: {sentence}")
            print(f"  Original Atomic Claim: {atom_text}")
            print(f"  Revised Atomic Claim: {revised_atom}")
            print(f"  Token Indices: {token_indices}")
            print(f"  Is Relevant: {is_relevant}")
            print(f"  Support Probability: {support_prob:.4f}")
            print(f"  Is Supported: {is_supported}")
            print("-" * 50)

            # Collect the result in a structured format
            result = {
                'sentence': sentence,
                'original_atomic_claim': atom_text,
                'revised_atomic_claim': revised_atom,
                'token_indices': token_indices,
                'is_relevant': is_relevant,
                'support_probability': support_prob,
                'is_supported': is_supported
            }
            all_results.append(result)
        return all_results

    def extract_atomic_facts(self, generation):
        """Extract atomic facts from a sentence along with token indices."""
        if self.af_generator is None:
            logging.debug("Initializing AtomicFactGenerator...")
            self.af_generator = AtomicFactGenerator(key_path=self.openai_key,
                                                    demon_dir=os.path.join(self.data_dir, "demos"),
                                                    gpt3_cache_file=os.path.join(self.cache_dir, "GPT4o.pkl"))
        # Use the atomic fact generator to get facts with token spans
        atomic_facts_pairs, _ = self.af_generator.run(generation)
        atomic_facts_with_spans = []
        for sent_text, facts in atomic_facts_pairs:
            for fact_text, token_indices in facts:
                atomic_facts_with_spans.append({
                    'fact': fact_text,
                    'token_indices': token_indices,
                    'sent_text': sent_text
                })
        return atomic_facts_with_spans

    def revise_fact(self, response, atomic_fact):
        # Use the model to revise the atomic fact
        full_prompt = _REVISE_FORMAT.replace(_STATEMENT_PLACEHOLDER, atomic_fact)
        full_prompt = full_prompt.replace(_RESPONSE_PLACEHOLDER, response)
        full_prompt = strip_string(full_prompt)
        logging.debug(f"Prompt for revising fact: {full_prompt}")

        try:
            model_response = self.relevance_lm.generate(full_prompt)
            logging.debug(f"Model response for revising fact: {model_response}")
            if isinstance(model_response, tuple):
                model_response = model_response[0]
            if not isinstance(model_response, str):
                logging.error(f"Model response is not a string: {type(model_response)}")
                model_response = None
        except Exception as e:
            logging.error(f"Error generating revised fact: {e}")
            model_response = None

        if model_response is None:
            logging.warning("Failed to obtain model response, using original atomic fact.")
            return atomic_fact

        revised_fact = extract_first_code_block(model_response, ignore_language=True)
        if revised_fact:
            return revised_fact.strip()
        else:
            # If the revision fails, use the original atomic fact
            logging.warning("Failed to revise atomic fact, using original.")
            return atomic_fact

    def check_relevance(self, prompt, response, atomic_fact):
        # Use the model to check if the atomic fact is relevant
        full_prompt = _RELEVANCE_FORMAT.replace(_STATEMENT_PLACEHOLDER, atomic_fact)
        full_prompt = full_prompt.replace(_PROMPT_PLACEHOLDER, prompt)
        full_prompt = full_prompt.replace(_RESPONSE_PLACEHOLDER, response)
        full_prompt = strip_string(full_prompt)
        logging.debug(f"Prompt for checking relevance: {full_prompt}")

        try:
            model_response = self.relevance_lm.generate(full_prompt)
            if isinstance(model_response, tuple):
                model_response = model_response[0]
            if not isinstance(model_response, str):
                logging.error(f"Model response is not a string: {type(model_response)}")
                return True  # Assume relevant if uncertain
        except Exception as e:
            logging.error(f"Error during relevance checking: {e}")
            return True  # Assume relevant if error occurs

        logging.debug(f"Model response for checking relevance: {model_response}")
        answer = extract_first_square_brackets(model_response)
        if answer:
            is_relevant = answer.strip().lower() == SYMBOL.lower()
            logging.debug(f"Extracted answer: {answer}, Is relevant: {is_relevant}")
            return is_relevant
        else:
            # If we cannot determine relevance, assume it's relevant
            logging.warning("Failed to determine relevance, assuming relevant.")
            return True

    def predict_support(self, atomic_fact):
        """Predict if the atomic fact is supported using the selected probe."""
        if not self.probe:
            raise ValueError("Probe model is not loaded.")
        if not hasattr(self, 'probe_type') or not self.probe_type:
            raise ValueError("Probe type not set. Use 'set_probe_type' method to set it before prediction.")
        
        # Tokenize the atomic fact
        inputs = self.tokenizer(atomic_fact, return_tensors='pt').to(self.device)

        # Forward pass to get hidden states
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
        hidden_states = outputs.hidden_states  # Tuple of hidden states from all layers

        # Extract the specified layers and concatenate them
        layer_start, layer_end = self.layer_group  # e.g., (13, 17)
        selected_hidden_states = hidden_states[layer_start:layer_end+1]  # +1 because the end index is inclusive

        # Handle token position
        if self.token_pos == 'lt':
            token_index = inputs['input_ids'].shape[1] - 1
        elif self.token_pos == 'slt':
            token_index = inputs['input_ids'].shape[1] - 2
        else:
            raise ValueError(f"Unsupported token position: {self.token_pos}")

        # Concatenate the selected hidden states for the specified token
        concatenated_hidden_state = torch.cat([h[:, token_index, :] for h in selected_hidden_states], dim=-1)
        # Shape: [batch_size, concatenated_hidden_size]

        # Convert to numpy
        concatenated_hidden_state_np = concatenated_hidden_state.cpu().numpy()

        # Use the selected probe to predict
        if self.classifier_name == 'logistic_regression':
            if not hasattr(self.probe, 'predict_proba'):
                raise AttributeError("Loaded probe does not have 'predict_proba' method.")
            support_prob = self.probe.predict_proba(concatenated_hidden_state_np)[0, 1]  # Probability of class '1'
        elif self.classifier_name == 'xgboost':
            if not hasattr(self.probe, 'predict_proba'):
                raise AttributeError("Loaded probe does not have 'predict_proba' method.")
            support_prob = self.probe.predict_proba(concatenated_hidden_state_np)[0, 1]  # Probability of class '1'
        else:
            raise ValueError(f"Unsupported classifier type: {self.classifier_name}")

        is_supported = support_prob > 0.5  # Threshold can be adjusted as needed
        logging.debug(f"Support Probability: {support_prob}, Is Supported: {is_supported}")
        return is_supported, support_prob

    def generate_response(self, prompt, max_new_tokens=512, temperature=0.5):
        """Generates a response to the prompt using the model, limited to 512 tokens."""
        # Tokenize the prompt
        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)
        
        # Generate response
        output = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,   # Limit to 512 tokens
            do_sample=True,                  # Enable sampling for more diverse responses
            temperature=temperature,         # Control randomness in sampling
            top_p=0.95,                      # Nucleus sampling to consider top 95% probability mass
            eos_token_id=self.tokenizer.eos_token_id  # Ensure proper end of sequence
        )
        
        # Decode the output tokens to get the generated text
        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
        
        # Extract the generated response (excluding the prompt)
        response = post_process_generation(generated_text[len(prompt):].strip())
        return response

def main():
    logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                        datefmt='%m/%d/%Y %H:%M:%S',
                        level=logging.INFO)

    # Initialize the FactProbeScorer with sample configurations
    fs = FactProbeScorer(
        model_name="retrieval+ChatGPT",
        data_dir=None,
        model_dir=None,
        cache_dir=None,
        openai_key="api.key",
        cost_estimate="consider_cache",
        abstain_detection_type=None,
        batch_size=256,
        probe_path="../../metrics/Llama3.1-8B_best_probe_layers_13-17_C_0.5.pkl",  # Replace with your probe file path
        hf_model_name='meta-llama/Meta-Llama-3.1-8B-Instruct',
        device='cuda',
        max_memory='80GIB'
    )

    # Example usage:
    topic = "Who is Michael Bronstein?"
    fs.set_probe_type('Logistic Regression')  # Set to 'XGBoost' as needed
    generation = fs.generate_response(topic)

    # Process the generation
    results = fs.process_generation(topic, generation)
    for res in results:
        print(res)

if __name__ == '__main__':
    main()
