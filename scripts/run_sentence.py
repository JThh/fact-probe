"""
To obtain the fact scores for the long-form generations, run the example command below:
python run_sentence.py --hf-model-name meta-llama/Llama-3.1-70B-Instruct --model-name Llama3.1-70B --entities-file ./FActScore/data/prompt_entities.txt --entity-range "100:" --max-entities 30 --verbose
"""

import logging
import os
import pickle
from tqdm import tqdm
import argparse
import random
import re

import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from factscore.factscorer import FactScorer

from openai import OpenAI

CLIENT = None
SUPPORTED_MODELS = [
    'meta-llama/Llama-3.1-70B-Instruct', 
    'meta-llama/Llama-3.1-8B-Instruct',
    'google/gemma-2-9b-it',
    'meta-llama/Llama-3.2-3B-Instruct',
    'openai:gpt-4o-mini-2024-07-18'
]

# Configure logging at the beginning of your script
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("generation.log"),
        logging.StreamHandler()
    ]
)

# Consolidated exclusion patterns using regex alternation and common keywords
exclusion_patterns = re.compile(
    r'\b(?:I\s+(?:cannot|was unable to|do not|have no)\s+(?:find|provide|locate|access)|'
    r'(?:No|Insufficient|Cannot|Unable)\s+(?:information|data|details|records|bio|profile|content)|'
    r'(?:I\s+am\s+not\s+sure|Note\s+(?:that|,))|'
    r'(?:No\s+(?:additional|further|relevant)\s+information))\b',
    flags=re.IGNORECASE
)

def load_entities(entities_file, entity_range=None, max_entities=None):
    """
    Load prompt entities from a file, optionally selecting a range and limiting the number.

    Args:
        entities_file (str): Path to the file containing entities.
        entity_range (str, optional): Slice string in the format 'start:end'.
        max_entities (int, optional): Maximum number of entities to load after applying the range.

    Returns:
        list: A list of selected entities.
    """
    with open(entities_file, 'r') as f:
        ents = f.read().splitlines()
        
    # random.seed(42)

    # indices = list(range(len(ents)))
    # random.Random(42).shuffle(indices)
    # ents = np.array(ents)[indices].tolist()

    # Apply range slicing if specified
    if entity_range:
        start, end = None, None
        parts = entity_range.split(':')
        if len(parts) == 2:
            start = int(parts[0]) if parts[0] else None
            end = int(parts[1]) if parts[1] else None
        elif len(parts) == 1:
            start = int(parts[0])
        ents = ents[start:end]
        logging.info(f'Applied entity range "{entity_range}": {len(ents)} entities selected.')

    # Apply max_entities limit if specified
    if max_entities:
        ents = ents[:max_entities]
        logging.info(f'Applied max_entities={max_entities}: {len(ents)} entities selected.')

    logging.info(f'Total entities loaded: {len(ents)}.')
    logging.info(f"Entities:", ents)
    return ents

def initialize_model(hf_model_name, device='cuda', max_memory='80GIB'):
    """Initialize the tokenizer and model."""
    if 'openai' in hf_model_name:
        global CLIENT
        CLIENT = OpenAI(api_key=os.environ['OPENAI_API_KEY'])
        logging.info(f'Model {hf_model_name} client initialized .')
        return None, None
    path = hf_model_name
    tokenizer = AutoTokenizer.from_pretrained(path)
    model = AutoModelForCausalLM.from_pretrained(
        path, 
        device_map="auto", 
        torch_dtype=torch.float16,
        cache_dir=os.environ["HF_DATASETS_CACHE"]
    )
    logging.info(f'Model "{hf_model_name}" loaded on device "{model.device}".')
    return tokenizer, model

def post_process_bio(generated_text: str, prompt: str) -> str:
    """
    Post-process the generated bio to ensure completeness and remove extraneous content.
    
    Args:
        generated_text (str): The raw text generated by the model.
        prompt (str): The prompt that was sent to the model.
    
    Returns:
        str: The cleaned and validated bio.
    """
    # Remove the prompt from the generated text
    bio = generated_text.replace(prompt, "").strip()
    # Ensure the bio ends with a period. If not, truncate to the last complete sentence.
    if not bio.endswith('.'):
        last_period = bio.rfind('.')
        if last_period != -1:
            bio = bio[:last_period + 1]
    
    # Remove any text after the first bio if multiple bios are generated
    bio = re.split(r'\nTell me a bio of', bio, flags=re.IGNORECASE)[0].strip()
    
    # Remove explanations about inability to find more information using the consolidated pattern
    bio = exclusion_patterns.split(bio)[0].strip()
    
    return bio

def generate_responses(ents, tokenizer, model, make_prompt, max_new_tokens, temperature, verbose=True, model_name=None):
    """Generate responses for each entity."""
    generations = []
    
    for ent in tqdm(ents, desc='Generating responses'):
        prompt = make_prompt(ent)
        if model is None:
            if isinstance(prompt, str):
                messages = [
                    {"role": "user", "content": prompt},
                ]
            else:
                messages = prompt
            output = CLIENT.chat.completions.create(
                model=model_name.split("openai:")[-1],  # openai:gpt-4o-mini-2024-07-18 -> gpt-4o-mini-2024-07-18
                messages=messages,
                max_tokens=max_new_tokens,
                temperature=0.0001,  # greedy
            )
            generated_text = output.choices[0].message.content
        else:
            inputs = tokenizer([prompt], return_tensors="pt").to('cuda')
            outputs = model.generate(
                **inputs, 
                max_new_tokens=max_new_tokens, 
                do_sample=False, 
                # temperature=temperature,
                eos_token_id=tokenizer.eos_token_id  # Ensure proper end of generation
            )
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
        # Apply post-processing to the generated text
        bio = post_process_bio(generated_text, prompt)
        generations.append(bio)

        if verbose:
            logging.info("\n" + "="*50)
            logging.info(f"Entity: {ent}")
            logging.info(f"Generated Response: {bio}")
            logging.info("="*50 + "\n")
    
    return generations

def compute_factscore(ents, generations, openai_key, gamma=10):
    """Compute FactScore for the generated responses."""
    fs = FactScorer(cache_dir='.')
    return fs.get_score(ents, generations, gamma=gamma, verbose=True)

def save_output(factscore_output, model_name, entity_range, temperature, max_new_tokens, api, output_dir, backup_dir, use_clamped):
    """Save the FactScore output to files."""
    os.makedirs(output_dir, exist_ok=True)
    
    from datetime import datetime
    formatted_time = datetime.now().strftime("%Y%m%d-%H%M%S")
    filename = f"{formatted_time}_{model_name}_{'clamped_' if use_clamped else ''}fact_scores_ent_{entity_range}_temp_{temperature}_maxtok_{max_new_tokens}_api_{api}.pkl"
    for dir_path in [output_dir]:
        with open(os.path.join(dir_path, filename), 'wb') as f:
            pickle.dump(factscore_output, f)
    logging.info(f'FactScore output saved to "{output_dir}".')

def get_postamble():
    """Return a string containing postamble for prompting."""
    postamble = """
Provide as many specific details and examples as possible (such as names of \
people, numbers, events, locations, dates, times, etc.). 
"""
    return postamble.strip()

def go_make_prompt(entity, postamble):
    """Construct a few-shot prompt with the given entity."""
    return f"Who is {entity}? {postamble}"

def graph_go_make_prompt(entity, postamble=None):
    return f"Tell me a paragraph bio of {entity}.\n"

def main():
    parser = argparse.ArgumentParser(description='FactScore Generation and Evaluation')
    parser.add_argument('--hf-model-name', type=str, default='meta-llama/Llama-3.1-70B-Instruct', help='Hugging Face model name, e.g. google/gemma-2-9b-it')
    parser.add_argument('--model-name', type=str, default=None, help='Model name identifier, e.g. Gemma2-9B')
    parser.add_argument('--entities-file', type=str, default='./FActScore/data/prompt_entities.txt', help='File containing entities')
    parser.add_argument('--max-new-tokens', type=int, default=512, help='Max number of new tokens to generate')  # Set to 128
    parser.add_argument('--temperature', type=float, default=0.0001, help='Sampling temperature')  # greedy
    parser.add_argument('--api', type=str, default='gpt-4o-mini', help='API used for FactScore. Not used.')
    parser.add_argument('--openai-key', type=str, help='OpenAI API key for FactScorer')
    parser.add_argument('--gamma', type=int, default=10, help='Gamma parameter for FactScorer')
    parser.add_argument('--device', type=str, default='cuda', help='Computation device')
    parser.add_argument('--max-memory', type=str, default='80GIB', help='Max memory per device for model')
    parser.add_argument('--output-dir', type=str, default='./FActScore/data/', help='Directory to save outputs')
    parser.add_argument('--backup-dir', type=str, default=None, help='Directory to save backup outputs')
    parser.add_argument('--use-clamped', action='store_true', help='Help distinguish file names when clamping')
    parser.add_argument('--entity-range', type=str, default=None, help='Range of entities to process, e.g., "100:", "100:200", ":200"')
    parser.add_argument('--max-entities', type=int, default=30, help='Maximum number of entities to process after applying the range')
    parser.add_argument('--verbose', action='store_true', help='Enable verbose output for generated responses')

    args = parser.parse_args()
    
    assert args.hf_model_name in [
        'meta-llama/Llama-3.1-70B-Instruct', 
        'meta-llama/Llama-3.1-8B-Instruct',
        'google/gemma-2-9b-it',
        'meta-llama/Llama-3.2-3B-Instruct',
        'openai:gpt-4o-mini-2024-07-18'
    ]
    
    ents = load_entities(args.entities_file, entity_range=args.entity_range, max_entities=args.max_entities)
    tokenizer, model = initialize_model(args.hf_model_name, device=args.device, max_memory=args.max_memory)
    
    postamble = get_postamble()
    make_prompt = lambda x: graph_go_make_prompt(x, postamble)
    
    generations = generate_responses(
        ents, 
        tokenizer, 
        model, 
        make_prompt, 
        args.max_new_tokens, 
        args.temperature,
        verbose=args.verbose,
        model_name=args.hf_model_name
    )
    
    factscore_output = compute_factscore(ents, generations, args.openai_key, gamma=args.gamma)
    
    if args.model_name is None:
        args.model_name = args.hf_model_name.split('/')[-1]

    save_output(
        factscore_output, 
        args.model_name, 
        args.entity_range if not args.entity_range else args.max_entities, 
        args.temperature, 
        args.max_new_tokens, 
        args.api, 
        args.output_dir, 
        args.backup_dir,
        args.use_clamped,
    )
    
    print('FactScore output saved successfully.')

if __name__ == '__main__':
    main()
